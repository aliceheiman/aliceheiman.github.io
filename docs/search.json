[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here are some of my favorite resources for those interested in AI for human and planetary health."
  },
  {
    "objectID": "resources.html#organizations",
    "href": "resources.html#organizations",
    "title": "Resources",
    "section": "Organizations",
    "text": "Organizations\n\nClimate Change AI: A non-profit and community committed to tackling climate change through machine learning. They host workshops, online happy hours, a community forum, and call for workshop papers at large AI conferences such as ICLR."
  },
  {
    "objectID": "resources.html#podcasts",
    "href": "resources.html#podcasts",
    "title": "Resources",
    "section": "Podcasts",
    "text": "Podcasts\n\nOutrage and Optimism: By Christiana Figueres and Tom Rivett-Carnac (oversaw the Paris Agreement), and Paul Dickinson (CDP founder) covering topics mainly in policy for a sustainable world."
  },
  {
    "objectID": "posts/precision_vs_recall.html",
    "href": "posts/precision_vs_recall.html",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "",
    "text": "I cannot count the number of times I‚Äôve had to Google the formulas for Precision and Recall, until I found an explanation that changed everything.\nThe explanation that made it click for me comes from the book Cloud-Based Remote Sensing with Google Earth Engine (Nicolau et al., 2023). Hopefully, this explanation will also be helpful to you.\nHere‚Äôs what we will build up to:"
  },
  {
    "objectID": "posts/precision_vs_recall.html#the-confusion-matrix",
    "href": "posts/precision_vs_recall.html#the-confusion-matrix",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "ü§î The Confusion Matrix",
    "text": "ü§î The Confusion Matrix\nRecall and precision are key metrics for evaluating classification models.\nLet‚Äôs say we have a grid of pixels we want a model to classify as (P)lant or (N)on-Vegetated area (our ‚ÄúPositive‚Äù and ‚ÄúNegative‚Äù classes). We will have actual (real-world) and predicted (model-classified) classifications. People often divide the predictions as follows:\n\nTrue Positive (TP): These are predicted positivies that are indeed actual positivies\nTrue Negative (TN): These are predicted negatives that are indeed actual negatives\nFalse Positive (FP): These are predicted positivies that are indeed actual negatives. They are not positives, although the model thought so!\nFalse Negative (FN): These are predicted negatives that are indeed actual positives. They are not negatives, although the model thought so!\n\nWe can summarize these combinations in a confusion matrix, like so:\n\n\n\nA Confusion Matrix\n\n\nNow we can understand recall and precision."
  },
  {
    "objectID": "posts/precision_vs_recall.html#recall-the-producers-accuracy",
    "href": "posts/precision_vs_recall.html#recall-the-producers-accuracy",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üèóÔ∏è Recall: The Producer‚Äôs Accuracy",
    "text": "üèóÔ∏è Recall: The Producer‚Äôs Accuracy\nRecall is the producer‚Äôs accuracy..\nThink of you creating a map of (P)lants and (N)on-vegetated areas. The recall for the (P)lant class is how many plants you can produce. This is just the number of correct (P)lants divided by all (P)lants. The correct number of plants is simply our true positive (TP) count. The total number of plants is the sum of the ones we classified correctly (TP) and the plants we wrongly misclassified, i.e., the false negative (FN) count.\n\\[\\text{Recall (P)} = \\frac{\\text{TP}}{\\text{TP + FN}}\\]\n\n\n\nRecall\n\n\nThink of it this way: there is some pool of actual positive pixels out there. When we are producing a map, we want to correctly classify (or recall) as many of those pixels as possible.\n\n\n\nAn Example of Recall"
  },
  {
    "objectID": "posts/precision_vs_recall.html#precision-the-users-accuracy",
    "href": "posts/precision_vs_recall.html#precision-the-users-accuracy",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üîé Precision: The User‚Äôs Accuracy",
    "text": "üîé Precision: The User‚Äôs Accuracy\nPrecision is the user‚Äôs accuracy..\nNow that the prediction map is complete, we are interested in the validity of the information as users of it. If the map says that this pixel is a (P)lant, how confident am I as a user that it is actually a plant?\nTherefore, we are interested in knowing the number of pixels correctly classified divided by the total number of pixels claimed to be in that class.\n\\[\\text{Precision (P)} = \\frac{\\text{TP}}{\\text{TP + FP}}\\]\n\n\n\nPrecision\n\n\nThink of it this way: the model claims that some pool of pixels is plants. However, you know that the model may have incorrectly labeled some pixels. So you want to know how much you can trust the plant pixel candidates.\n\n\n\nAn Example of Precision"
  },
  {
    "objectID": "posts/precision_vs_recall.html#accuracy-total-assessment",
    "href": "posts/precision_vs_recall.html#accuracy-total-assessment",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "‚öôÔ∏è Accuracy: Total Assessment",
    "text": "‚öôÔ∏è Accuracy: Total Assessment\nNow, the original accuracy is the number of correctly classified pixels (both positive and negative) divided by the total number of pixels.\n\\[\\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + FP + TN + FN}} = \\frac{\\text{TP + TN}}{\\text{Total}}\\]\n\n\n\nAccuracy\n\n\nThis is a general metric, but it may not provide insights into class imbalances."
  },
  {
    "objectID": "posts/precision_vs_recall.html#summary",
    "href": "posts/precision_vs_recall.html#summary",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üìù Summary",
    "text": "üìù Summary\nSo there we have it: precision, recall, and accuracy.\n\n\n\nPrecision vs.¬†Recall vs.¬†Accuracy\n\n\nHopefully, you now have a better understanding of these terms. Perfect recall implies that you, as a producer, can correctly label every single positive class pixel. However, this doesn‚Äôt necessarily make users happy if the model labels everything as the positive class.\nDifferent applications will require varying degrees of attentiveness to precision and recall. Applications where a misclassification could be lethal benefit from high precision standards. However, if it is costly to miss even a single instance, you might be more interested in recall.\nThank you for reading!"
  },
  {
    "objectID": "posts/precision_vs_recall.html#references",
    "href": "posts/precision_vs_recall.html#references",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üóÉÔ∏è References",
    "text": "üóÉÔ∏è References\nNicolau, A. P., Dyson, K., Saah, D., & Clinton, N. (2023). Accuracy Assessment: Quantifying Classification Quality. In J. Cardille, M. Crowley, D. Saah, & N. Clinton (Eds.), Cloud-Based Remote Sensing with Google Earth Engine: Fundamentals and Applications (pp.¬†135‚Äì145). SpringerOpen"
  },
  {
    "objectID": "posts/alphaearth-intro.html",
    "href": "posts/alphaearth-intro.html",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "",
    "text": "AlphaEarth Embeddings of a Point in Brazil, State of Mato Grosso"
  },
  {
    "objectID": "posts/alphaearth-intro.html#why-alphaearth",
    "href": "posts/alphaearth-intro.html#why-alphaearth",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "Why AlphaEarth?",
    "text": "Why AlphaEarth?\nFor millenia, humans have stared into the night sky curious about space - but quite recently we‚Äôve gotten the tools to look back onto the Earth.\nHowever, the amount of data has become a challenge. In the 1970‚Äôs, the Landsat 1-3 satelittes on-board storage capacity was 3.75 GB per orbit. Today, Landsat 7 and 8 collect 1,200 scenes, equivalent to 1 TB of data, every 24 hours1, adding 3 TB of landsat products2.\nAlphaEarth Foundations is Google DeepMind‚Äôs latest geospatial foundational AI model trained to assimilate thousands of image bands\nThe model is trained on 3 billion individual image frames sampled from over 5 million locations globally3. Some of the data bands include\n\nOptical and thermal imagery\nRadar data\n3D surface measurements\nClimate properties\nGravity fields\nGeo-located descriptive text\n‚Ä¶and more\n\nThrough training, the model can compress all of this data into 64 numbers for each 10mx10m location, ready for downstream analysis.\nThe Google DeepMind team open released the Satellite Embedding V1 Dataset, which contains annual 64-dimensional embeddings between 2017-2024 at a 10m resolution4."
  },
  {
    "objectID": "posts/alphaearth-intro.html#what-are-satellite-embeddings",
    "href": "posts/alphaearth-intro.html#what-are-satellite-embeddings",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "What are Satellite Embeddings?",
    "text": "What are Satellite Embeddings?\nIn essence, the thousands of data bands are compressed into 64 bands that together is called a satellite embeddings.\n\n\n\nThe 64 bands of the satellite embedding visualized for a point in Brazil.\n\n\nYou can think of the embeddings as new satellite embedding coordinates that locates each point on Earth on a 64-dimensional sphere. These embeddings are constructed such that similar points (such as locations with solar panels) have embeddings pointing to locations close together on the sphere, while different points (such as solar panels vs.¬†forest) have embeddings that point to different locations on the sphere.\n\n\nVideo\nSimilar locations have satellite embeddings close together, while different locations have embeddings that point in different directions.\n\n\nWhat‚Äôs special is that the embeddings capture similarities and difference both across space and time.\nThis means that it is possible to track changes in the same patch of land just by looking at the evolution of the embedding.\n\n\nVideo\nThe same geographical point can get different satellite embeddings over time, making it possible to track changes over time.\n\n\nThe indication of similarity between two spatio-temporal locations is then the angle between the satellite embedding coordinates.\nA popular similarity metric is the cosine similarity score, taking the cosine of the angle between the vectors.\n\\[\n\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}||\\mathbf{b}|}\n\\]\nThe embedding coordinates in the AlphaEarth Foundation Satellite Embeddings dataset have already been normalized to have unit length (a length of one). Therefore, the cosine similarity simplifies to just the dot product between the embedding vectors. This is very efficient to compute.\n\\[\n\\cos(\\theta) = \\mathbf{a} \\cdot \\mathbf{b}\n\\]\nVisualizing the cosine similarity, it is possible to highlight areas that are either very similar or very different.\n\n\nVideo\nThe dot product between two satellite embedding vector coordinates measures the similarity between them."
  },
  {
    "objectID": "posts/alphaearth-intro.html#what-can-we-use-satellite-embeddings-for",
    "href": "posts/alphaearth-intro.html#what-can-we-use-satellite-embeddings-for",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "What can we use Satellite Embeddings for?",
    "text": "What can we use Satellite Embeddings for?\nSince the satellite embeddings contain a combination of thousands of data layers, they contain very rich semantic information about the relationship between locations on Earth.\nIn fact, AlphaEarth achieves state of the art results on many benchmarks, with the authors repororting on average 24% lower error rates compared to other models tested5.\nSome downstream tasks highlighted by the authors include similarity search, unsupervised clustering, and supervised classification3.\n\n\n\nAlphaEarth Foundations and the Satellite Embedding Dataset enable several downstream tasks, such as similarity search, unsupervised clustering, and supervised classification\n\n\nMoreover, what makes these embeddings so powerful is that they support continuous time6.\nTraditional data products are discrete in time due to satellite only passing over a patch of land at discrete points in time. But AlphaEarth is trained to interpolate between these.\nThese tasks can help with applications such as tracking deforestation and urban expansion, categorizing agricultural lands to aid in food security, and help model water resources."
  },
  {
    "objectID": "posts/alphaearth-intro.html#what-are-the-limitations",
    "href": "posts/alphaearth-intro.html#what-are-the-limitations",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "What are the limitations?",
    "text": "What are the limitations?\nThe Satellite Embedding dataset still has its fair share of limitations, including:\n\nLess interpretable features: The new 64 bands don‚Äôt have a direct physical meaning. This means it can be hard to interpret predictions and feature importance.\nOnly 2017-2024: The V1 dataset only goes back to 2017, limiting studies going further back in time.\nMostly land-focused: Many original data sources have limited data over the ocean, also impacting the embeddings.\nLimited coverage at the poles: The dataset also has limited quality around the poles."
  },
  {
    "objectID": "posts/alphaearth-intro.html#resources",
    "href": "posts/alphaearth-intro.html#resources",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "Resources",
    "text": "Resources\nIf you are curious to learn more, the Google DeepMind team has curated a collection of excellent resources and tutorials.\nMoreover, the Google DeepMind team is currenlty offering a series of small grants to researchers using the Satellite Embedding dataset."
  },
  {
    "objectID": "posts/alphaearth-intro.html#conclusion",
    "href": "posts/alphaearth-intro.html#conclusion",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nThe AlphaEarth Foundations geospatial foundation model marks a pivotal moment for geospatial analysis.\nI am very excited to explore the applications of this model!\nTake care! ü•≥"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome! Coding for Human and Planetary Health",
    "section": "",
    "text": "Hey Friends!\nI invite you on a journey to explore the exciting intersection of coding for human and planetary health.\nOn this site, you will find\n\nTechnical deepdives: Dive into Machine Learning (ML) and Artificial Intelligence (AI) methods applied to everything from forest health prediction to MRI analysis.\nBook Notes: Get notes and summaries of key books and papers in AI+Nature+Health.\nUpdates: Stay up to date with exciting research, initiatives, and projects in AI for human and planetary health.\n\nAll of this is to explore the questions how can we practically use coding for sustainable development, while minimizing the negative social and environmental impacts of AI?\nExcited to learn more?\nCheck out the Resources and the Latest Articles.\nTake care!\n\n\nReferences\nCover Photo by Ian Wetherill on Unsplash"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest Articles",
    "section": "",
    "text": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n\nAlice Heiman\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nJul 28, 2025\n\n\nAlice Heiman\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome! Coding for Human and Planetary Health\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nJul 17, 2025\n\n\nAlice Heiman\n\n\n\n\n\nNo matching items"
  }
]