[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here are some of my favorite resources for those interested in AI for human and planetary health."
  },
  {
    "objectID": "resources.html#organizations",
    "href": "resources.html#organizations",
    "title": "Resources",
    "section": "Organizations",
    "text": "Organizations\n\nClimate Change AI: A non-profit and community committed to tackling climate change through machine learning. They host workshops, online happy hours, a community forum, and call for workshop papers at large AI conferences such as ICLR."
  },
  {
    "objectID": "resources.html#podcasts",
    "href": "resources.html#podcasts",
    "title": "Resources",
    "section": "Podcasts",
    "text": "Podcasts\n\nOutrage and Optimism: By Christiana Figueres and Tom Rivett-Carnac (oversaw the Paris Agreement), and Paul Dickinson (CDP founder) covering topics mainly in policy for a sustainable world."
  },
  {
    "objectID": "posts/precision_vs_recall.html",
    "href": "posts/precision_vs_recall.html",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "",
    "text": "I cannot count the number of times I‚Äôve had to Google the formulas for Precision and Recall, until I found an explanation that changed everything.\nThe explanation that made it click for me comes from the book Cloud-Based Remote Sensing with Google Earth Engine (Nicolau et al., 2023). Hopefully, this explanation will also be helpful to you.\nHere‚Äôs what we will build up to:"
  },
  {
    "objectID": "posts/precision_vs_recall.html#the-confusion-matrix",
    "href": "posts/precision_vs_recall.html#the-confusion-matrix",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "ü§î The Confusion Matrix",
    "text": "ü§î The Confusion Matrix\nRecall and precision are key metrics for evaluating classification models.\nLet‚Äôs say we have a grid of pixels we want a model to classify as (P)lant or (N)on-Vegetated area (our ‚ÄúPositive‚Äù and ‚ÄúNegative‚Äù classes). We will have actual (real-world) and predicted (model-classified) classifications. People often divide the predictions as follows:\n\nTrue Positive (TP): These are predicted positivies that are indeed actual positivies\nTrue Negative (TN): These are predicted negatives that are indeed actual negatives\nFalse Positive (FP): These are predicted positivies that are indeed actual negatives. They are not positives, although the model thought so!\nFalse Negative (FN): These are predicted negatives that are indeed actual positives. They are not negatives, although the model thought so!\n\nWe can summarize these combinations in a confusion matrix, like so:\n\n\n\nA Confusion Matrix\n\n\nNow we can understand recall and precision."
  },
  {
    "objectID": "posts/precision_vs_recall.html#recall-the-producers-accuracy",
    "href": "posts/precision_vs_recall.html#recall-the-producers-accuracy",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üèóÔ∏è Recall: The Producer‚Äôs Accuracy",
    "text": "üèóÔ∏è Recall: The Producer‚Äôs Accuracy\nRecall is the producer‚Äôs accuracy..\nThink of you creating a map of (P)lants and (N)on-vegetated areas. The recall for the (P)lant class is how many plants you can produce. This is just the number of correct (P)lants divided by all (P)lants. The correct number of plants is simply our true positive (TP) count. The total number of plants is the sum of the ones we classified correctly (TP) and the plants we wrongly misclassified, i.e., the false negative (FN) count.\n\\[\\text{Recall (P)} = \\frac{\\text{TP}}{\\text{TP + FN}}\\]\n\n\n\nRecall\n\n\nThink of it this way: there is some pool of actual positive pixels out there. When we are producing a map, we want to correctly classify (or recall) as many of those pixels as possible.\n\n\n\nAn Example of Recall"
  },
  {
    "objectID": "posts/precision_vs_recall.html#precision-the-users-accuracy",
    "href": "posts/precision_vs_recall.html#precision-the-users-accuracy",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üîé Precision: The User‚Äôs Accuracy",
    "text": "üîé Precision: The User‚Äôs Accuracy\nPrecision is the user‚Äôs accuracy..\nNow that the prediction map is complete, we are interested in the validity of the information as users of it. If the map says that this pixel is a (P)lant, how confident am I as a user that it is actually a plant?\nTherefore, we are interested in knowing the number of pixels correctly classified divided by the total number of pixels claimed to be in that class.\n\\[\\text{Precision (P)} = \\frac{\\text{TP}}{\\text{TP + FP}}\\]\n\n\n\nPrecision\n\n\nThink of it this way: the model claims that some pool of pixels is plants. However, you know that the model may have incorrectly labeled some pixels. So you want to know how much you can trust the plant pixel candidates.\n\n\n\nAn Example of Precision"
  },
  {
    "objectID": "posts/precision_vs_recall.html#accuracy-total-assessment",
    "href": "posts/precision_vs_recall.html#accuracy-total-assessment",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "‚öôÔ∏è Accuracy: Total Assessment",
    "text": "‚öôÔ∏è Accuracy: Total Assessment\nNow, the original accuracy is the number of correctly classified pixels (both positive and negative) divided by the total number of pixels.\n\\[\\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + FP + TN + FN}} = \\frac{\\text{TP + TN}}{\\text{Total}}\\]\n\n\n\nAccuracy\n\n\nThis is a general metric, but it may not provide insights into class imbalances."
  },
  {
    "objectID": "posts/precision_vs_recall.html#summary",
    "href": "posts/precision_vs_recall.html#summary",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üìù Summary",
    "text": "üìù Summary\nSo there we have it: precision, recall, and accuracy.\n\n\n\nPrecision vs.¬†Recall vs.¬†Accuracy\n\n\nHopefully, you now have a better understanding of these terms. Perfect recall implies that you, as a producer, can correctly label every single positive class pixel. However, this doesn‚Äôt necessarily make users happy if the model labels everything as the positive class.\nDifferent applications will require varying degrees of attentiveness to precision and recall. Applications where a misclassification could be lethal benefit from high precision standards. However, if it is costly to miss even a single instance, you might be more interested in recall.\nThank you for reading!"
  },
  {
    "objectID": "posts/precision_vs_recall.html#references",
    "href": "posts/precision_vs_recall.html#references",
    "title": "Precision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference",
    "section": "üóÉÔ∏è References",
    "text": "üóÉÔ∏è References\nNicolau, A. P., Dyson, K., Saah, D., & Clinton, N. (2023). Accuracy Assessment: Quantifying Classification Quality. In J. Cardille, M. Crowley, D. Saah, & N. Clinton (Eds.), Cloud-Based Remote Sensing with Google Earth Engine: Fundamentals and Applications (pp.¬†135‚Äì145). SpringerOpen"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome! Coding for Human and Planetary Health",
    "section": "",
    "text": "Hey Friends!\nI invite you on a journey to explore the exciting intersection of coding for human and planetary health.\nOn this site, you will find\n\nTechnical deepdives: Dive into Machine Learning (ML) and Artificial Intelligence (AI) methods applied to everything from forest health prediction to MRI analysis.\nBook Notes: Get notes and summaries of key books and papers in AI+Nature+Health.\nUpdates: Stay up to date with exciting research, initiatives, and projects in AI for human and planetary health.\n\nAll of this is to explore the questions how can we practically use coding for sustainable development, while minimizing the negative social and environmental impacts of AI?\nExcited to learn more?\nCheck out the Resources and the Latest Articles.\nTake care!\n\n\nReferences\nCover Photo by Ian Wetherill on Unsplash"
  },
  {
    "objectID": "posts/alphaearth-intro.html",
    "href": "posts/alphaearth-intro.html",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "",
    "text": "AlphaEarth Embeddings of a Point in Brazil, State of Mato Grosso"
  },
  {
    "objectID": "posts/alphaearth-intro.html#why-alphaearth",
    "href": "posts/alphaearth-intro.html#why-alphaearth",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "Why AlphaEarth?",
    "text": "Why AlphaEarth?\nFor millenia, humans have stared into the night sky curious about space - but quite recently we‚Äôve gotten the tools to look back onto the Earth.\nHowever, the amount of data has become a challenge. In the 1970‚Äôs, the Landsat 1-3 satelittes on-board storage capacity was 3.75 GB per orbit. Today, Landsat 7 and 8 collect 1,200 scenes, equivalent to 1 TB of data, every 24 hours1, adding 3 TB of landsat products2.\nAlphaEarth Foundations is Google DeepMind‚Äôs latest geospatial foundational AI model trained to assimilate thousands of image bands\nThe model is trained on 3 billion individual image frames sampled from over 5 million locations globally3. Some of the data bands include\n\nOptical and thermal imagery\nRadar data\n3D surface measurements\nClimate properties\nGravity fields\nGeo-located descriptive text\n‚Ä¶and more\n\nThrough training, the model can compress all of this data into 64 numbers for each 10mx10m location, ready for downstream analysis.\nThe Google DeepMind team open released the Satellite Embedding V1 Dataset, which contains annual 64-dimensional embeddings between 2017-2024 at a 10m resolution4."
  },
  {
    "objectID": "posts/alphaearth-intro.html#what-are-satellite-embeddings",
    "href": "posts/alphaearth-intro.html#what-are-satellite-embeddings",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "What are Satellite Embeddings?",
    "text": "What are Satellite Embeddings?\nIn essence, the thousands of data bands are compressed into 64 bands that together is called a satellite embeddings.\n\n\n\nThe 64 bands of the satellite embedding visualized for a point in Brazil.\n\n\nYou can think of the embeddings as new satellite embedding coordinates that locates each point on Earth on a 64-dimensional sphere. These embeddings are constructed such that similar points (such as locations with solar panels) have embeddings pointing to locations close together on the sphere, while different points (such as solar panels vs.¬†forest) have embeddings that point to different locations on the sphere.\n\n\nVideo\nSimilar locations have satellite embeddings close together, while different locations have embeddings that point in different directions.\n\n\nWhat‚Äôs special is that the embeddings capture similarities and difference both across space and time.\nThis means that it is possible to track changes in the same patch of land just by looking at the evolution of the embedding.\n\n\nVideo\nThe same geographical point can get different satellite embeddings over time, making it possible to track changes over time.\n\n\nThe indication of similarity between two spatio-temporal locations is then the angle between the satellite embedding coordinates.\nA popular similarity metric is the cosine similarity score, taking the cosine of the angle between the vectors.\n\\[\n\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}||\\mathbf{b}|}\n\\]\nThe embedding coordinates in the AlphaEarth Foundation Satellite Embeddings dataset have already been normalized to have unit length (a length of one). Therefore, the cosine similarity simplifies to just the dot product between the embedding vectors. This is very efficient to compute.\n\\[\n\\cos(\\theta) = \\mathbf{a} \\cdot \\mathbf{b}\n\\]\nVisualizing the cosine similarity, it is possible to highlight areas that are either very similar or very different.\n\n\nVideo\nThe dot product between two satellite embedding vector coordinates measures the similarity between them."
  },
  {
    "objectID": "posts/alphaearth-intro.html#what-can-we-use-satellite-embeddings-for",
    "href": "posts/alphaearth-intro.html#what-can-we-use-satellite-embeddings-for",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "What can we use Satellite Embeddings for?",
    "text": "What can we use Satellite Embeddings for?\nSince the satellite embeddings contain a combination of thousands of data layers, they contain very rich semantic information about the relationship between locations on Earth.\nIn fact, AlphaEarth achieves state of the art results on many benchmarks, with the authors repororting on average 24% lower error rates compared to other models tested5.\nSome downstream tasks highlighted by the authors include similarity search, unsupervised clustering, and supervised classification3.\n\n\n\nAlphaEarth Foundations and the Satellite Embedding Dataset enable several downstream tasks, such as similarity search, unsupervised clustering, and supervised classification\n\n\nMoreover, what makes these embeddings so powerful is that they support continuous time6.\nTraditional data products are discrete in time due to satellite only passing over a patch of land at discrete points in time. But AlphaEarth is trained to interpolate between these.\nThese tasks can help with applications such as tracking deforestation and urban expansion, categorizing agricultural lands to aid in food security, and help model water resources."
  },
  {
    "objectID": "posts/alphaearth-intro.html#what-are-the-limitations",
    "href": "posts/alphaearth-intro.html#what-are-the-limitations",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "What are the limitations?",
    "text": "What are the limitations?\nThe Satellite Embedding dataset still has its fair share of limitations, including:\n\nLess interpretable features: The new 64 bands don‚Äôt have a direct physical meaning. This means it can be hard to interpret predictions and feature importance.\nOnly 2017-2024: The V1 dataset only goes back to 2017, limiting studies going further back in time.\nMostly land-focused: Many original data sources have limited data over the ocean, also impacting the embeddings.\nLimited coverage at the poles: The dataset also has limited quality around the poles."
  },
  {
    "objectID": "posts/alphaearth-intro.html#resources",
    "href": "posts/alphaearth-intro.html#resources",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "Resources",
    "text": "Resources\nIf you are curious to learn more, the Google DeepMind team has curated a collection of excellent resources and tutorials.\nMoreover, the Google DeepMind team is currenlty offering a series of small grants to researchers using the Satellite Embedding dataset."
  },
  {
    "objectID": "posts/alphaearth-intro.html#conclusion",
    "href": "posts/alphaearth-intro.html#conclusion",
    "title": "AlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nThe AlphaEarth Foundations geospatial foundation model marks a pivotal moment for geospatial analysis.\nI am very excited to explore the applications of this model!\nTake care! ü•≥"
  },
  {
    "objectID": "posts/plant_a_pr/index.html",
    "href": "posts/plant_a_pr/index.html",
    "title": "Plant A PR: Open-Source Sprint for Sustainability",
    "section": "",
    "text": "Sustainability-focused open-source projects play a critical role in developing tools for human and planetary health, but many struggle to attract consistent contributors.\nAt the same time, students and software developers are eager to make a meaningful impact, yet often lack clear entry points for contributing in small, traceable ways. While hackathons can foster innovation and community, they are frequently speculative and focused on demos rather than shipped code.\nThis short post outlines a focused alternative.\nSimilar to a mapathon, participants work individually or in small groups to complete clearly scoped issues in existing repositories. Projects can be sourced from Climate Triage with clear ‚ÄúGood First Issues.‚Äù\nOne pull request (PR) = one tree planted in the digital world\nBy contributing to established projects, participants create immediate, real-world impact while learning practical open-source collaboration skills."
  },
  {
    "objectID": "posts/plant_a_pr/index.html#the-plant-a-pr-event-format",
    "href": "posts/plant_a_pr/index.html#the-plant-a-pr-event-format",
    "title": "Plant A PR: Open-Source Sprint for Sustainability",
    "section": "The Plant-A-PR Event Format",
    "text": "The Plant-A-PR Event Format\nIntroduction (5-10 minutes) - Overview of the Plant a PR concept and the pre-selected open-source projects for the sprint.\nOpen-Source Crash Course (15-20 minutes) - Guided walkthrough of GitHub fundamentals, including issues, forking repositories, and submitting pull requests.\nContribution Sprint (1-3 hours) - Participants work solo or in small teams to complete and submit as many pull requests as possible.\nConclusion (5 minutes) - Group wrap-up highlighting total pull requests submitted, reflections, and key learnings."
  },
  {
    "objectID": "posts/plant_a_pr/index.html#looking-ahead",
    "href": "posts/plant_a_pr/index.html#looking-ahead",
    "title": "Plant A PR: Open-Source Sprint for Sustainability",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nThe Plant-A-PR event format can be used to gather participants interested in the intersection of software and sustainability. Similar to tree planting initiatives, communities such as schools can establish leaderboards to motivate participants."
  },
  {
    "objectID": "posts/plant_a_pr/index.html#references",
    "href": "posts/plant_a_pr/index.html#references",
    "title": "Plant A PR: Open-Source Sprint for Sustainability",
    "section": "References",
    "text": "References\n\nCover photo by Wolfgang Hasselmann on Unsplash. Available at https://unsplash.com/photos/a-red-tree-in-the-middle-of-a-purple-field-3dyIfpZLHEA?utm_source=unsplash."
  },
  {
    "objectID": "posts/ai_not_fix_climate_change/index.html",
    "href": "posts/ai_not_fix_climate_change/index.html",
    "title": "Don‚Äôt Wait for AI to ‚ÄúFix‚Äù the Climate Crisis",
    "section": "",
    "text": "Today, Artificial Intelligence (AI) is presented as either the definitive solution to the world‚Äôs problems or the ultimate end of life on Earth.\nWe allow AIs‚Äôunchecked growth, arguing that once we solve Intelligence, we‚Äôll solve everything else. AI will be able to find cures to all diseases, predict earthquakes, and optimize our energy grids. Just bet everything on AI, and we‚Äôll have a world of guaranteed human flourishing.\nBut believing in only one of two outcomes, AI utopia (infinite human flourishing) or AI dystopia (complete human extinction), prevents us from taking action on the matters we must address today.\nThe utopia-dystopia debate is especially dangerous for climate solutions, where the promises of future technology to fix our historical mistakes stand in the way of taking the actions we desperately need today.\nAlthough I strongly believe AI can play a crucial role in the transition to a greener planet, in this article, I argue why we cannot bet on AI alone to solve climate change."
  },
  {
    "objectID": "posts/ai_not_fix_climate_change/index.html#believing-in-ai-fixes-is-too-software-centric",
    "href": "posts/ai_not_fix_climate_change/index.html#believing-in-ai-fixes-is-too-software-centric",
    "title": "Don‚Äôt Wait for AI to ‚ÄúFix‚Äù the Climate Crisis",
    "section": "Believing in AI fixes is too software-centric",
    "text": "Believing in AI fixes is too software-centric\nUnlike software innovations, hardware and infrastructure changes take decades, not days.\nEspecially for climate solutions, pure software solutions tend to forget physical reality. AI can help at many stages of a hardware solution‚Äôs life cycle. From developing carbon-neutral materials, finding optimal power plant locations, optimizing drifting, and predicting maintenance schedules and failure modes. But you still don‚Äôt get away from the complicated logistics of physical and interconnected systems.\nLet‚Äôs say you suddenly had the recipe for a hyper-efficient nuclear reactor. Even then, it would still take decades to develop the infrastructure and expertise to deploy, integrate, operate, and service the system. Unlike a vibe-coded website, transforming energy systems, rolling out universal vaccination programs, and ensuring equitable access to quality education cannot be done in hours, or even months. These challenges leave no room for waiting on perfect solutions. Action is already long overdue.\nThe best projects are often the ones that actually get done. Endless optimization means little without public and political will, sustained financing, and effective implementation. Without those, even the most powerful models and predictions never get realized."
  },
  {
    "objectID": "posts/ai_not_fix_climate_change/index.html#we-repeatedly-overestimate-our-ability-to-develop-meaningful-ai",
    "href": "posts/ai_not_fix_climate_change/index.html#we-repeatedly-overestimate-our-ability-to-develop-meaningful-ai",
    "title": "Don‚Äôt Wait for AI to ‚ÄúFix‚Äù the Climate Crisis",
    "section": "We repeatedly overestimate our ability to develop meaningful AI",
    "text": "We repeatedly overestimate our ability to develop meaningful AI\nSome thought machine intelligence would be solved 70 years ago, in one single summer.\nKnown as the 1956 Dartmouth summer project, John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon famously proposed that ‚Äúevery aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. [‚Ä¶] We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.‚Äù\nNow, 70 years later, we have still not ‚Äúsolved‚Äù Intelligence, whatever that means.\nRather, we are debating and constraining the definition of Intelligence to move the goalposts closer in the competition to reach Artificial General Intelligence (AGI) first. But, more importantly for climate change, regardless of the definition, what is the path from Intelligence to real-world progress in human and planetary development?\nHere, I wish companies focused less on beating benchmarks and more on demonstrating real intent to address the world‚Äôs biggest challenges, by allocating budgets and investment toward climate change, agriculture, education, and health."
  },
  {
    "objectID": "posts/ai_not_fix_climate_change/index.html#we-dont-need-ai-to-take-action",
    "href": "posts/ai_not_fix_climate_change/index.html#we-dont-need-ai-to-take-action",
    "title": "Don‚Äôt Wait for AI to ‚ÄúFix‚Äù the Climate Crisis",
    "section": "We don‚Äôt need AI to take action",
    "text": "We don‚Äôt need AI to take action\nBy trusting a future AI system more than our current understanding, we are undermining the hard-earned wealth of knowledge and experience of indigenous communities and environmental scientists.\nWe already have innovations and a roadmap for what needs to be done to combat climate change, including transforming our energy systems from fossil fuels to renewables and nuclear power, increasing crop yields, eating less meat, and stopping deforestation. Often, the main bottleneck is not the need for better technology but rather the need to roll out existing solutions on a larger scale.\nIn the age of AI, it‚Äôs easy to forget how innovative the human mind can be. Long before the AI boom, we put a person on the Moon, cured once-fatal diseases with antibiotics, and built transportation and energy systems that connected the world. With the same collective determination, we can achieve equally transformative breakthroughs in addressing climate change."
  },
  {
    "objectID": "posts/ai_not_fix_climate_change/index.html#conclusion",
    "href": "posts/ai_not_fix_climate_change/index.html#conclusion",
    "title": "Don‚Äôt Wait for AI to ‚ÄúFix‚Äù the Climate Crisis",
    "section": "Conclusion",
    "text": "Conclusion\nArtificial Intelligence can undoubtedly have a large positive impact on our transition towards a more sustainable world.\nBut believing that this technology will be able to fix climate change and reverse our historical damages is outright dangerous because it hinders us from taking long-overdue action, putting more people at risk every day.\nSo don‚Äôt wait for AI to fix the climate.\nEmbrace what new technology provides as an addition, not a replacement. We already know what we need to get started."
  },
  {
    "objectID": "posts/ai_not_fix_climate_change/index.html#references",
    "href": "posts/ai_not_fix_climate_change/index.html#references",
    "title": "Don‚Äôt Wait for AI to ‚ÄúFix‚Äù the Climate Crisis",
    "section": "References",
    "text": "References\nCover photo by Logan Voss on Unsplash\nAI Utopia / Dystopia Graph from CS182 Lecture Slides 1, accessed February 01, 2026."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest Articles",
    "section": "",
    "text": "Plant A PR: Open-Source Sprint for Sustainability\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nFeb 8, 2026\n\n\nAlice Heiman\n\n\n\n\n\n\n\n\n\n\n\n\nDon‚Äôt Wait for AI to ‚ÄúFix‚Äù the Climate Crisis\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nFeb 1, 2026\n\n\nAlice Heiman\n\n\n\n\n\n\n\n\n\n\n\n\nAlphaEarth: A Peek into the Potential of Geospatial Satelitte Embeddings\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n\nAlice Heiman\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision vs.¬†Recall vs.¬†Accuracy: Finally Memorize the Difference\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nJul 28, 2025\n\n\nAlice Heiman\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome! Coding for Human and Planetary Health\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nJul 17, 2025\n\n\nAlice Heiman\n\n\n\n\n\nNo matching items"
  }
]