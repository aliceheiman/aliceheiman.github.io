{
 "cells": [
  {
   "cell_type": "raw",
   "id": "88169b8e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Precision vs. Recall vs. Accuracy: Finally Memorize the Difference\"\n",
    "author: \"Alice Heiman\"\n",
    "date: \"2025-07-28\"\n",
    "image: \"precision_vs_recall/summary.png\"\n",
    "categories: [Article]\n",
    "toc: true\n",
    "citation:                                   # Include \"how to site this article\"\n",
    "  url: https://aliceheiman.github.io\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c167bb3",
   "metadata": {},
   "source": [
    "I cannot count the number of times I've had to Google the formulas for *Precision* and *Recall*, **until I found an explanation that changed everything**.\n",
    "\n",
    "The explanation that made it click for me comes from the book [Cloud-Based Remote Sensing with Google Earth Engine](https://docs.google.com/document/d/1UCB900oCdJERca-2WUeDlCu52MjPKJxETJ_jJcLM0bM/edit?tab=t.0) (Nicolau et al., 2023). Hopefully, this explanation will also be helpful to you.\n",
    "\n",
    "Here's what we will build up to:\n",
    "\n",
    "![Precision vs. Recall vs. Accuracy](precision_vs_recall/summary.png)\n",
    "\n",
    "## ü§î The Confusion Matrix\n",
    "\n",
    "Recall and precision are key metrics for evaluating **classification** models.\n",
    "\n",
    "Let's say we have a grid of pixels we want a model to classify as (P)lant or (N)on-Vegetated area (our \"Positive\" and \"Negative\" classes). We will have **actual** (real-world) and **predicted** (model-classified) classifications. People often divide the predictions as follows:\n",
    "\n",
    "* **True Positive (TP)**: These are **predicted positivies** that are indeed **actual positivies**\n",
    "\n",
    "* **True Negative (TN)**: These are **predicted negatives** that are indeed **actual negatives**\n",
    "\n",
    "* **False Positive (FP)**: These are **predicted positivies** that are indeed **actual negatives**. They are not positives, although the model thought so!\n",
    "\n",
    "* **False Negative (FN)**: These are **predicted negatives** that are indeed **actual positives**. They are not negatives, although the model thought so!\n",
    "\n",
    "We can summarize these combinations in a **confusion matrix**, like so:\n",
    "\n",
    "![A Confusion Matrix](precision_vs_recall/confusion-matrix.png)\n",
    "\n",
    "**Now we can understand recall and precision.**\n",
    "\n",
    "## üèóÔ∏è Recall: The Producer's Accuracy\n",
    "\n",
    "**Recall is the producer's accuracy.**. \n",
    "\n",
    "Think of you creating a map of (P)lants and (N)on-vegetated areas. The recall for the (P)lant class is how many plants you can produce. This is just the number of correct (P)lants divided by all (P)lants. The correct number of plants is simply our true positive (TP) count. The total number of plants is the sum of the ones we classified correctly (TP) and the plants we wrongly misclassified, i.e., the false negative (FN) count.\n",
    "\n",
    "$$\\text{Recall (P)} = \\frac{\\text{TP}}{\\text{TP + FN}}$$\n",
    "\n",
    "![Recall](precision_vs_recall/recall.png)\n",
    "\n",
    "Think of it this way: there is some pool of actual positive pixels out there. When we are producing a map, we want to correctly classify (or recall) as many of those pixels as possible.\n",
    "\n",
    "![An Example of Recall](precision_vs_recall/recall-example.png)\n",
    "\n",
    "## üîé Precision: The User's Accuracy\n",
    "\n",
    "**Precision is the user's accuracy.**. \n",
    "\n",
    "Now that the prediction map is complete, we are interested in the validity of the information **as users of it**. If the map says that this pixel is a (P)lant, how confident am I **as a user** that it is actually a plant?\n",
    "\n",
    "Therefore, we are interested in knowing the number of pixels correctly classified divided by the total number of pixels claimed to be in that class.\n",
    "\n",
    "$$\\text{Precision (P)} = \\frac{\\text{TP}}{\\text{TP + FP}}$$\n",
    "\n",
    "![Precision](precision_vs_recall/precision.png)\n",
    "\n",
    "Think of it this way: the model claims that some pool of pixels is plants. However, you know that the model may have incorrectly labeled some pixels. So you want to know how much you can trust the plant pixel candidates.\n",
    "\n",
    "![An Example of Precision](precision_vs_recall/precision-example.png)\n",
    "\n",
    "## ‚öôÔ∏è Accuracy: Total Assessment\n",
    "\n",
    "Now, the original accuracy is the number of correctly classified pixels (both positive and negative) divided by the total number of pixels.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + FP + TN + FN}} = \\frac{\\text{TP + TN}}{\\text{Total}}$$\n",
    "\n",
    "![Accuracy](precision_vs_recall/accuracy.png)\n",
    "\n",
    "This is a general metric, but it may not provide insights into class imbalances.\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "So there we have it: precision, recall, and accuracy. \n",
    "\n",
    "![Precision vs. Recall vs. Accuracy](precision_vs_recall/summary.png)\n",
    "\n",
    "Hopefully, you now have a better understanding of these terms. Perfect recall implies that you, as a producer, can correctly label every single positive class pixel. However, this doesn't necessarily make users happy if the model labels everything as the positive class.\n",
    "\n",
    "Different applications will require varying degrees of attentiveness to precision and recall. Applications where a misclassification could be lethal benefit from high precision standards. However, if it is costly to miss even a single instance, you might be more interested in recall.\n",
    "\n",
    "Thank you for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e8af0",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è References\n",
    "\n",
    "Nicolau, A. P., Dyson, K., Saah, D., & Clinton, N. (2023). Accuracy Assessment: Quantifying Classification Quality. In J. Cardille, M. Crowley, D. Saah, & N. Clinton (Eds.), Cloud-Based Remote Sensing with Google Earth Engine: Fundamentals and Applications (pp. 135‚Äì145). SpringerOpen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013caf3e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
